{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3522ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7dfa18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 32 threads\n",
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set device and CPU threads\n",
    "from multiprocessing import cpu_count\n",
    "torch.set_num_threads(cpu_count())\n",
    "print(f'Using {torch.get_num_threads()} threads')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40ca5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_VOICE_TOKEN = 2100\n",
    "MIN_TOKEN = 3600\n",
    "MAX_TOKEN = 8400\n",
    "VALID_TOKENS = set()\n",
    "for v in range(MIN_TOKEN, MAX_TOKEN + 1, 10):\n",
    "    VALID_TOKENS.add(v)\n",
    "# NUM_SECONDS seconds of data * around 87 pitch readings per second\n",
    "NUM_SECONDS = 10\n",
    "BLOCK_SIZE = 87 * NUM_SECONDS\n",
    "NOISE_WINDOW_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2891ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS NAMES - NOTE: Do not change numbering.\n",
    "CLASS_NAMES = {\n",
    "    'saveri': 0,\n",
    "    'hemavati': 1,\n",
    "    'thodi': 2,\n",
    "    'sindhubhairavi': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2830bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PitchDataFile:\n",
    "    file_path: str\n",
    "    pitches: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048abbd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Hop Length: 435\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/trichur-hemavati-alapana_C#3_minus_1\n",
      "pitches length: 41884\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/trichur-hemavati-alapana_C#3_plus_1\n",
      "pitches length: 43699\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/aruna-hemavati_F3\n",
      "pitches length: 106804\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/aruna-hemavati_F3_plus_3\n",
      "pitches length: 102215\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/aruna-hemavati_F3_minus_1\n",
      "pitches length: 99673\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/aruna-hemavati_F3_minus_2\n",
      "pitches length: 99591\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/trichur-hemavati-alapana_C#3_plus_3\n",
      "pitches length: 43829\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/trichur-hemavati-alapana_C#3\n",
      "pitches length: 49792\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/aruna-hemavati_F3_plus_2\n",
      "pitches length: 101721\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/aruna-hemavati_F3_plus_1\n",
      "pitches length: 101094\n",
      "processing ../../data/simple-test/pitch_data_midi/hemavati/trichur-hemavati-alapana_C#3_plus_2\n",
      "pitches length: 43187\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/vishweshwar-darshan-abhishek-raghuram_C#3_minus_1\n",
      "pitches length: 56297\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/karunai-deivame-sudha_G#3_plus_2\n",
      "pitches length: 10840\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/karunai-deivame-sudha_G#3\n",
      "pitches length: 11915\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/thillana-bombay-jayashree_G#3_plus_3\n",
      "pitches length: 3810\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/karunai-deivame-sudha_G#3_minus_2\n",
      "pitches length: 10623\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/vishweshwar-darshan-abhishek-raghuram_C#3_plus_1\n",
      "pitches length: 57068\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/vishweshwar-darshan-abhishek-raghuram_C#3_plus_3\n",
      "pitches length: 57975\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/vishweshwar-darshan-abhishek-raghuram_C#3_plus_2\n",
      "pitches length: 57060\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/karunai-deivame-sudha_G#3_minus_1\n",
      "pitches length: 10825\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/thillana-bombay-jayashree_G#3_minus_1\n",
      "pitches length: 3584\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/thillana-bombay-jayashree_G#3_plus_1\n",
      "pitches length: 3816\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/thillana-bombay-jayashree_G#3\n",
      "pitches length: 3736\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/karunai-deivame-sudha_G#3_plus_3\n",
      "pitches length: 11159\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/karunai-deivame-sudha_G#3_plus_1\n",
      "pitches length: 10736\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/vishweshwar-darshan-abhishek-raghuram_C#3\n",
      "pitches length: 60938\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/thillana-bombay-jayashree_G#3_plus_2\n",
      "pitches length: 3643\n",
      "processing ../../data/simple-test/pitch_data_midi/sindhubhairavi/thillana-bombay-jayashree_G#3_minus_2\n",
      "pitches length: 3511\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/dasarathi-tnseshagopalan_C3_plus_3\n",
      "pitches length: 70763\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/dasarathi-tnseshagopalan_C3\n",
      "pitches length: 76678\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/era-napai-ranjani-gayatri_G#3\n",
      "pitches length: 6263\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/kavava-bombay-jayashree_G#3_plus_2\n",
      "pitches length: 17604\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/kavava-bombay-jayashree_G#3_minus_1\n",
      "pitches length: 17708\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/kavava-bombay-jayashree_G#3_minus_2\n",
      "pitches length: 17156\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/era-napai-ranjani-gayatri_G#3_minus_1\n",
      "pitches length: 5661\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/dasarathi-tnseshagopalan_C3_plus_2\n",
      "pitches length: 70520\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/dasarathi-tnseshagopalan_C3_plus_1\n",
      "pitches length: 69275\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/thodi-tmk_C#3_minus_1\n",
      "pitches length: 43465\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/kavava-bombay-jayashree_G#3_plus_3\n",
      "pitches length: 18030\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/era-napai-ranjani-gayatri_G#3_plus_3\n",
      "pitches length: 5910\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/kavava-bombay-jayashree_G#3_plus_1\n",
      "pitches length: 17817\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/thodi-tmk_C#3\n",
      "pitches length: 46453\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/era-napai-ranjani-gayatri_G#3_plus_1\n",
      "pitches length: 5869\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/kavava-bombay-jayashree_G#3\n",
      "pitches length: 20198\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/thodi-tmk_C#3_plus_3\n",
      "pitches length: 44995\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/thodi-tmk_C#3_plus_2\n",
      "pitches length: 44431\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/era-napai-ranjani-gayatri_G#3_plus_2\n",
      "pitches length: 5871\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/thodi-tmk_C#3_plus_1\n",
      "pitches length: 44336\n",
      "processing ../../data/simple-test/pitch_data_midi/thodi/era-napai-ranjani-gayatri_G#3_minus_2\n",
      "pitches length: 5614\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/nedunuri-amma-nannubrovave_A3_minus_1\n",
      "pitches length: 7911\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/sanjay-alapana_D3_plus_3\n",
      "pitches length: 39295\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/arunasairam-muruga-muruga_F3_minus_2\n",
      "pitches length: 18007\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/bombaysis-balamu-kulamu_F#3_plus_2\n",
      "pitches length: 8374\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/nedunuri-amma-nannubrovave_A3_plus_1\n",
      "pitches length: 8377\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/nedunuri-amma-nannubrovave_A3_minus_2\n",
      "pitches length: 8093\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/bombaysis-balamu-kulamu_F#3_plus_1\n",
      "pitches length: 8248\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/arunasairam-muruga-muruga_F3\n",
      "pitches length: 20297\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/nedunuri-amma-nannubrovave_A3_plus_2\n",
      "pitches length: 7842\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/sanjay-alapana_D3_minus_2\n",
      "pitches length: 37752\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/amritha-murali-parashakti-manuparada_G3_minus_1\n",
      "pitches length: 50401\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/bombaysis-balamu-kulamu_F#3_minus_2\n",
      "pitches length: 8213\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/amritha-murali-parashakti-manuparada_G3_plus_2\n",
      "pitches length: 51029\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/arunasairam-muruga-muruga_F3_plus_1\n",
      "pitches length: 18545\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/amritha-murali-parashakti-manuparada_G3_plus_1\n",
      "pitches length: 50744\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/bombaysis-balamu-kulamu_F#3_plus_3\n",
      "pitches length: 8390\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/bombaysis-balamu-kulamu_F#3_minus_1\n",
      "pitches length: 8287\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/sanjay-alapana_D3_minus_1\n",
      "pitches length: 38196\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/sanjay-alapana_D3\n",
      "pitches length: 41642\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/arunasairam-muruga-muruga_F3_minus_1\n",
      "pitches length: 19059\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/arunasairam-muruga-muruga_F3_plus_2\n",
      "pitches length: 18883\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/bombaysis-balamu-kulamu_F#3\n",
      "pitches length: 8800\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/amritha-murali-parashakti-manuparada_G3\n",
      "pitches length: 56486\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/sanjay-alapana_D3_plus_2\n",
      "pitches length: 39111\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/sanjay-alapana_D3_plus_1\n",
      "pitches length: 38753\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/arunasairam-muruga-muruga_F3_plus_3\n",
      "pitches length: 18483\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/nedunuri-amma-nannubrovave_A3\n",
      "pitches length: 8657\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/amritha-murali-parashakti-manuparada_G3_plus_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitches length: 50646\n",
      "processing ../../data/simple-test/pitch_data_midi/saveri/amritha-murali-parashakti-manuparada_G3_minus_2\n",
      "pitches length: 49992\n",
      "Total data: 5891\n"
     ]
    }
   ],
   "source": [
    "# For initial tests, keep entire data in memory. Keep only the pitch list.\n",
    "pitch_data_dir = '../../data/simple-test/pitch_data_midi'\n",
    "\n",
    "X = []\n",
    "pitch_data = {}\n",
    "# 10 second segments with overlap of 5 seconds to previous segment\n",
    "SAMPLE_HOP_LENGTH = BLOCK_SIZE // 2 \n",
    "print(f'Sample Hop Length: {SAMPLE_HOP_LENGTH}')\n",
    "\n",
    "num_invalid = 0\n",
    "\n",
    "pitch_counter = defaultdict(int)\n",
    "\n",
    "for class_name in os.listdir(pitch_data_dir):\n",
    "    if class_name not in CLASS_NAMES:\n",
    "        print(f'{class_name} not included for training')\n",
    "        continue\n",
    "    class_dir = os.path.join(pitch_data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        data = []\n",
    "        print(f'processing {file_path}')\n",
    "        with open(file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                if len(row) >= 3:\n",
    "                    data.append(row[2])\n",
    "            ########################################################\n",
    "            ## NOTE ################################################\n",
    "            ## Remove noise token - add noise padding on sides instead\n",
    "            #########################################################\n",
    "            pitches = []\n",
    "            for x in data:\n",
    "                ix = int(x)\n",
    "                if ix in VALID_TOKENS:\n",
    "                    pitches.append(ix)\n",
    "                elif ix != NOT_VOICE_TOKEN:\n",
    "                    num_invalid += 1\n",
    "            for v in pitches:\n",
    "                pitch_counter[v] += 1\n",
    "            #pitches = [stoi[NOT_VOICE_TOKEN]] * NOISE_WINDOW_SIZE + pitches + [stoi[NOT_VOICE_TOKEN]] * NOISE_WINDOW_SIZE\n",
    "            print(f'pitches length: {len(pitches)}')\n",
    "            #########################################################\n",
    "            pd = PitchDataFile(file_path=file_path, pitches=pitches)\n",
    "            sampling_data = [(file_path, i, CLASS_NAMES[class_name]) for i in range(0, len(pitches) - BLOCK_SIZE - 1, SAMPLE_HOP_LENGTH)]\n",
    "            X.extend(sampling_data)\n",
    "            pitch_data[file_path] = pd\n",
    "\n",
    "print(f'Total data: {len(X)}')\n",
    "assert num_invalid == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274a5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchDataset(Dataset):\n",
    "    def __init__(self, data_samples: List[Tuple], pitch_data: Dict[str, PitchDataFile], \n",
    "                 block_size: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_samples: List of tuples containing (file_path, start_index, class_label)\n",
    "            pitch_data: Dictionary mapping file paths to PitchDataFile objects\n",
    "            block_size: Size of each pitch sequence block\n",
    "            device: torch device to store tensors on\n",
    "        \"\"\"\n",
    "        self.data_samples = data_samples\n",
    "        self.pitch_data = pitch_data\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, start_index, class_label = self.data_samples[idx]\n",
    "        pitch_sequence = self.pitch_data[file_path].pitches[start_index:start_index + self.block_size]\n",
    "        return (torch.tensor(pitch_sequence, device=self.device, dtype=torch.int),\n",
    "                torch.tensor(class_label, device=self.device))\n",
    "\n",
    "def create_pitch_datasets(X: List[Tuple], pitch_data: Dict[str, PitchDataFile], \n",
    "                         block_size: int, device: torch.device, \n",
    "                         train_size: float = 0.8, val_size: float = 0.1,\n",
    "                         random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test datasets with computed class weights.\n",
    "    \n",
    "    Args:\n",
    "        X: List of (file_path, start_index, class_label) tuples\n",
    "        pitch_data: Dictionary mapping file paths to PitchDataFile objects\n",
    "        block_size: Size of each pitch sequence block\n",
    "        device: torch device to store tensors on\n",
    "        train_size: Proportion of data to use for training\n",
    "        val_size: Proportion of data to use for validation\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset, class_weights\n",
    "    \"\"\"\n",
    "    # First split into train and temp\n",
    "    train_data, temp_data = train_test_split(X, train_size=train_size, \n",
    "                                           random_state=random_state)\n",
    "    \n",
    "    # Then split temp into validation and test\n",
    "    val_ratio = val_size / (1 - train_size)\n",
    "    val_data, test_data = train_test_split(temp_data, train_size=val_ratio,\n",
    "                                         random_state=random_state)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PitchDataset(train_data, pitch_data, block_size, device)\n",
    "    val_dataset = PitchDataset(val_data, pitch_data, block_size, device)\n",
    "    test_dataset = PitchDataset(test_data, pitch_data, block_size, device)\n",
    "    \n",
    "    # Compute class weights\n",
    "    y_train = [sample[2] for sample in train_data]\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                                       classes=np.unique(y_train),\n",
    "                                       y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, device=device, dtype=torch.float32)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, class_weights\n",
    "\n",
    "# Usage example:\n",
    "def create_data_loaders(X, pitch_data, block_size, device, random_state, batch_size=32):\n",
    "    # Create datasets\n",
    "    train_dataset, val_dataset, test_dataset, class_weights = create_pitch_datasets(\n",
    "        X, pitch_data, block_size, device, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec36792",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, class_weights = create_data_loaders(X, pitch_data, BLOCK_SIZE, device, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed5d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, out_channels, n_embd, n_tokens, hidden_size, num_layers, device='cpu', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.n_embd = n_embd\n",
    "        self.n_tokens = n_tokens\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.emb = nn.Embedding(n_tokens, n_embd, device=device)\n",
    "        self.lstm = nn.LSTM(input_size=n_embd, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, device=device)\n",
    "        self.task = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 100, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(100, out_channels, device=device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        # x shape: (batch_size, sequence_length, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # Take the last hidden state of the LSTM\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        output = self.task(last_hidden_state)\n",
    "        return output\n",
    "\n",
    "    def class_params(self):\n",
    "        return {\n",
    "            'out_channels': self.out_channels,\n",
    "            'n_embd': self.n_embd,\n",
    "            'n_tokens': self.n_tokens,\n",
    "            'dropout': self.dropout,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_layers': self.num_layers\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c92a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path:str, model, optimizer, class_params, epochs, train_loss, val_loss):\n",
    "    torch.save({\n",
    "        'epochs': epochs,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'class_params': class_params\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f156cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model\n",
      "Total number of parameters: 434776\n",
      "Epochs trained so far: 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "out_channels = len(CLASS_NAMES)  # Output channels (e.g., regression output)\n",
    "n_tokens = len(VALID_TOKENS)\n",
    "n_embd = 96\n",
    "\n",
    "# For LSTM\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-3\n",
    "epochs_so_far = [0]\n",
    "model = LSTMNet(out_channels, n_embd, n_tokens, hidden_size, num_layers, device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "IMPORT_PATH = '../models/lstm-midi-1-epochs-[200000]'\n",
    "MODEL_PATH = '../models/lstm-midi-1'\n",
    "if os.path.exists(IMPORT_PATH):\n",
    "    print('Model exists: Loading')\n",
    "    #MODEL_PATH = '../models/cnn-5-no-noise-token-1e3'\n",
    "    checkpoint = torch.load(IMPORT_PATH)\n",
    "    epochs_so_far[0] = checkpoint['epochs']\n",
    "    train_loss = checkpoint['train_loss']\n",
    "    val_loss = checkpoint['val_loss']\n",
    "    print(f'checkpoint after epoch: {epochs_so_far[0]}')\n",
    "    print(f'train loss: {train_loss}')\n",
    "    print(f'val loss: {val_loss}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "else:\n",
    "    print(f'Creating new model')\n",
    "    \n",
    "model.train()\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "tr_losses = []\n",
    "v_losses = []\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f'Epochs trained so far: {epochs_so_far[0]}')\n",
    "ud = []\n",
    "# Example input - First dimension is number of minibatches, second is embedding dimension, third is context size\n",
    "#input_tensor = torch.randint(0, n_tokens, (32, 2700), device=device)  # Batch size, channels, sequence length\n",
    "#logits = model(input_tensor)\n",
    "#loss = F.cross_entropy(logits, torch.randint(0, out_channels, (32,), device=device))\n",
    "#print(f'output shape {output.shape}')\n",
    "#print(f'logits: {logits.shape}')\n",
    "#print(f'loss: {loss}')\n",
    "#total_params = sum(param.numel() for param in model.parameters())\n",
    "#print(f'total params {total_params}')\n",
    "\n",
    "#for p in model.parameters():\n",
    "#    p.grad = None\n",
    "#loss.backward()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b7a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs_so_far, max_steps):\n",
    "    it = iter(train_loader)\n",
    "    for i in range(0, max_steps):\n",
    "        start_time = time.time()\n",
    "        # minibatch construct\n",
    "        Xb, Yb = next(it)\n",
    "        #Xb, Yb = next(data_iterator)\n",
    "        # forward pass\n",
    "        logits = model(Xb)\n",
    "        loss = F.cross_entropy(logits, Yb, weight=class_weights) # loss function\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track stats\n",
    "        epochs_so_far[0] += 1\n",
    "        if i % 1000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        #lossi.append(loss.log10().item())\n",
    "        #with torch.no_grad():\n",
    "        #    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ec2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(epochs):\n",
    "    max_steps = 5\n",
    "    N = 10\n",
    "    for _ in range(N):\n",
    "        model.train()\n",
    "        train(epochs, max_steps)\n",
    "        model.eval()\n",
    "        train_loss = split_loss('train')\n",
    "        val_loss = split_loss('val')\n",
    "        #train_loss, val_loss = 0,0\n",
    "        tr_losses.append(train_loss)\n",
    "        v_losses.append(val_loss)\n",
    "        print(f'Train loss: {train_loss}, val loss: {val_loss}')\n",
    "        save_model(MODEL_PATH + '-epochs-' + str(epochs), model, optimizer, model.class_params(), epochs_so_far[0], tr_losses[-1], v_losses[-1])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(torch.tensor(tr_losses), label='Training Loss', color='blue')\n",
    "    plt.plot(torch.tensor(v_losses), label='Validation Loss', color='red')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Losses Over Time')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19136bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [36,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_so_far\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mloop\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m split_loss(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs_so_far, max_steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m Xb, Yb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(it)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Xb, Yb = next(data_iterator)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Yb, weight\u001b[38;5;241m=\u001b[39mclass_weights) \u001b[38;5;66;03m# loss function\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mLSTMNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# x shape: (batch_size, sequence_length, embedding_dim)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# lstm_out shape: (batch_size, sequence_length, hidden_size)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Take the last hidden state of the LSTM\u001b[39;00m\n\u001b[1;32m     29\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "loop(epochs_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8387a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
